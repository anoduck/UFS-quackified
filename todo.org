#+TITLE: Ultimate Facebook Scraper
#+DATE: Sun Dec 22 21:35:37 2019
#+PROJECT: Development
#+TAGS: research(r) development(d) testing(t)
#+TODO: TODO MAYBE SOMEDAY | DONE CANCEL
#+PRIORITY: A B C D
#+OPTIONS: H:3 num:nil toc:nil \n:nil ::t |:t ^:t -:t f:T *:T
#+EXPORT_SELECT_TAGS: EXPORT
#+EXPORT_EXCLUDE_TAGS: noexport
#+STARTUP: align nodlcheck nofold oddeven hidestars
#+DRAWERS: PROPERTIES CLOCK LOGBOOK RESULTS FEEDSTATUS
#+COLUMNS: %38ITEM(Details) %TAGS(Context) %7SCHEDULED(Planned) %7TODO(To Do) %5PRIORITY(PRIORITY) %5DONE(Completeness){X%} %5Effort(Time){:} %6CLOCKSUM(Total){:}
# ---------------------------------------------------------------
* Tasks
** TODO [#C] Test running script with mobile useragent               :research:
    DEADLINE: <2020-01-12 21:30>
    :PROPERTIES
    :Effort: 1h
    :END:
** SOMEDAY [#C] Develop means to multiplex process              :development:
** TODO [#A] Successfully test a scraping to detect rate limiting prevention             :testing:
    SCHEDULED: <2019-12-23 19:23>
    :PROPERTIES:
    :Effort: 1h
    :END:
* Feature Requests
*** TODO filter users by sex
*** TODO Scrape people who are tagged in photos
*** TODO add ratelimit to dependencies in setup.py:
    install_requires=["selenium==3.141.0", "pyyaml", "ratelimit"],
*** CANCEL remove requirements.txt?
    seems useless, dependencies are defined in setup.py
*** MAYBE suggest shallow clone for download = git clone --depth=1
*** MAYBE coding style: prefer "absolute names" for imported objects.
    for example, instead of from random import randint; randint() use import random; random.randint() etc. the "ugly method" is slightly faster, but "explicit namespaces are much more readable" src. also we can better distinguish between "internal" and "exernal" functions.
*** TODO replace selenium with selenium-wire to read/write http headers
    project and sample. for example, with selenium, i cannot add application/json to the client's accept header, which would be useful in some exotic situations : P also better for testing and monitoring, e.g. monitor that chromium is not leaking private data
*** TODO only download platform-specific binaries.
    currently i get three versions for linux64, mac64 and win32 -- code sample
*** MAYBE move all configuration to one central object, including xpath selector strings, image size config [large or small images], login data, input list .... allow to change config for every input item, for example "only download text data from this profile" or "download all except photos from this profile"
*** CANCEL keep track of state, use a sqlite database, flush often [commit database changes to disk often, so we dont lose data -- no problem for SSD drives]. tolerate unhandled exceptions [program crashes], allow to resume from last position, avoid re-downloading
*** CANCEL behave more "natural" and "human" while scraping .... this is probably the hardest part cos "what is human?" except solving captchas. facebook aint stupid, they will monitor this project, and train their defense systems to block our software. what i think of "as last resort" is a browser plugin, where you manually browse a profile, and the plugin saves all the data you visit, and also helps to "browse more" data, like "expand all comments". with that user behavior, the program can "learn" to act more human .... to apply a similar "browsing strategy" automatically to other profiles .... machine learning to the rescue. for that purpose, we might have to rewrite the code to javascript : /
*** CANCEL better visualize the scraping process, for easier monitoring and debugging .... like a "side by side" comparison between original and copy
*** CANCEL better visualize the result. offer a local http server so we can "browse offline". instead of "endless scroll" use "old school pagination", like 100 posts per page, next page, last page, go to page X, etc. .... also let me filter results by time, location, mood, keywords, tags etc.
*** TODO use firefox over chrome
*** TODO Move from serial approach to parallel approach
    ps, regarding "keep track of state", this is some horrible code:

            if save_status != 3:
                scroll()

            data = driver.find_elements_by_xpath(elements_path[i])

            save_to_file(file_names[i], data, save_status, i)

    this is the "serial" approach, or "depth first search"

        scroll the page all the way down
        find all elements
        save all data

    this must be rewritten to a "parallel" approach using yield statements,
    so when one function fails (scroll or find), for whatever reason,
    we at least have all the data to this point, and can try to resume/debug later

    also the "serial" approach is a huge waste of memory
    currently the scraper needs more than 4 gigabytes of ram